{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚡ Lightning + WandB - Custom Scratch Models (CNN & Fully Connected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "\n",
    "import wandb\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities.model_summary import ModelSummary\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import torchmetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maryangarg019\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder Utilities ----------------------------\n",
    "\n",
    "## Create dir if it doesn't exist\n",
    "def create_dir(dir_name):\n",
    "  if not os.path.exists(f'{dir_name}'):\n",
    "    os.mkdir(f'{dir_name}')\n",
    "\n",
    "## Delete dir: checkpoints\n",
    "def delete_dir(dir_name):\n",
    "  if os.path.isdir(f'{dir_name}'):\n",
    "    shutil.rmtree(f'{dir_name}')\n",
    "\n",
    "# ---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: For experiments later on!\n",
    "# train_transform = A.Compose(\n",
    "#     [\n",
    "#         A.SmallestMaxSize(max_size=160),\n",
    "#         A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "#         A.RandomCrop(height=128, width=128),\n",
    "#         A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
    "#         A.RandomBrightnessContrast(p=0.5),\n",
    "#         A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "#         ToTensorV2(),\n",
    "#     ]\n",
    "# )\n",
    "train_transform = T.Compose([T.ToTensor(), T.Resize((224, 224))])\n",
    "test_transform = T.Compose([T.ToTensor(), T.Resize((224, 224))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'mode': 'train',\n",
    "    'train_path': 'dataset_FER/train/',\n",
    "    'test_path': 'dataset_FER/test/',\n",
    "    'epochs': 1,\n",
    "    'batch_size': 16,\n",
    "    'lr': 0.001,\n",
    "    'num_workers': 4,\n",
    "    'device': 'cuda',\n",
    "    'device_ids': [0,1],\n",
    "    'load_model': False,\n",
    "    'checkpoint_path': 'ckpts/scratch/',\n",
    "    'save_every': 10,\n",
    "    'device': 'cuda:1' if torch.cuda.is_available() else 'cpu',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.ImageFolder(root='dataset_FER/train/', transform=train_transform)\n",
    "test_data = os.listdir('dataset_FER/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoader = DataLoader(train_data, batch_size=16, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 224, 224]),\n",
       " tensor([4, 5, 0, 4, 2, 2, 4, 3, 2, 5, 3, 3, 2, 3, 6, 3]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample, y = next(iter(trainLoader))\n",
    "sample.shape, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.scratch_cnn import ScratchCNN\n",
    "from models.scratch_fully_connected import FullyConnected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn = ScratchCNN()\n",
    "# fcn.to('cuda')\n",
    "\n",
    "# from torchsummary import summary\n",
    "# summary(fcn, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcn = FullyConnected(input_size = 3*224*224, hidden_size = 1024, num_classes = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIT Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIT_Scratch(pl.LightningModule):\n",
    "  \n",
    "  def __init__(self, model):\n",
    "    super().__init__()\n",
    "    self.model = model\n",
    "    self.save_hyperparameters()\n",
    "\n",
    "    self.roc = torchmetrics.ROC(task='multiclass', num_classes=7)\n",
    "    self.acc = torchmetrics.Accuracy(task='multiclass', num_classes=7)\n",
    "    self.auroc = torchmetrics.classification.MulticlassAUROC(num_classes=7)\n",
    "    self.f1 = torchmetrics.F1Score(task='multiclass', num_classes=7)\n",
    "\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    optimizer = torch.optim.AdamW(self.parameters(), lr=1e-3)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "  def forward(self, z):\n",
    "    return self.model(z)\n",
    "\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    imgs, y = batch\n",
    "    y_hat = self(imgs)\n",
    "    loss = F.cross_entropy(y_hat, y)\n",
    "\n",
    "    self.log('train_loss_CE', loss, prog_bar=True)\n",
    "    \n",
    "    # np_y = y.clone().detach().view(-1).cpu().numpy()\n",
    "    # np_yhat = y_hat.clone().detach().view(-1).cpu().numpy()\n",
    "\n",
    "    # metrics\n",
    "    self.log('train_acc', self.acc(y_hat, y), prog_bar=True)\n",
    "    self.log('train_F1', self.f1(y_hat, y), prog_bar=False)\n",
    "\n",
    "    self.roc.update(y_hat, y)\n",
    "    # print(f\"{batch_idx+1} ROC --> FPR: {fpr} | TPR: {tpr}\")\n",
    "\n",
    "    # self.log({\"ROC\" : wandb.plot.roc_curve(np_y, np_yhat, \\\n",
    "    #   labels=[\"Angry\",\"Disgust\",\"Fear\",\"Happy\",\"Neutral\",\"Sad\",\"Surprise\"], classes_to_plot=None)}\n",
    "    #   , prog_bar=False)\n",
    "    \n",
    "    # wandb.sklearn.plot_confusion_matrix(np_y, np_yhat, [\"Angry\",\"Disgust\",\"Fear\",\"Happy\",\"Neutral\",\"Sad\",\"Surprise\"]) \n",
    "    \n",
    "    # self.log({\"Precision-Recall\": wandb.plot.pr_curve(np_y, np_yhat, \\\n",
    "    #   labels=[\"Angry\",\"Disgust\",\"Fear\",\"Happy\",\"Neutral\",\"Sad\",\"Surprise\"], classes_to_plot=None)})\n",
    "\n",
    "    self.log('train_AUROC', self.auroc(y_hat, y), prog_bar=False)\n",
    "\n",
    "    return loss\n",
    "    \n",
    "\n",
    "  def on_train_epoch_end(self):\n",
    "    print( f\"ROC: {self.roc.compute()}\" )\n",
    "    self.roc.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Callback\n",
    "from pytorch_lightning.callbacks import DeviceStatsMonitor, TQDMProgressBar, ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=CONFIG['checkpoint_path'],\n",
    "                                      filename='{epoch}-{train_loss_CE:.2f}',\n",
    "                                      monitor='train_loss_CE',\n",
    "                                      save_top_k=2,\n",
    "                                      save_last=True,\n",
    "                                      save_weights_only=True,\n",
    "                                      verbose=True,\n",
    "                                      mode='min')\n",
    "\n",
    "# Exp2: Learning Rate Monitor\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step', log_momentum=True)\n",
    "\n",
    "# Earlystopping\n",
    "# earlystopping = EarlyStopping(monitor='train_loss_CE', patience=3, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a1669b8ba84630930091e11519ebab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669319182013472, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230801_124830-rpn5iood</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aryangarg019/MMU-FER/runs/rpn5iood' target=\"_blank\">FCN-HS-1024_ROC</a></strong> to <a href='https://wandb.ai/aryangarg019/MMU-FER' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aryangarg019/MMU-FER' target=\"_blank\">https://wandb.ai/aryangarg019/MMU-FER</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aryangarg019/MMU-FER/runs/rpn5iood' target=\"_blank\">https://wandb.ai/aryangarg019/MMU-FER/runs/rpn5iood</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(project='MMU-FER', \n",
    "                           name='FCN-HS-1024_ROC',\n",
    "                           config=CONFIG,\n",
    "                           job_type='train',\n",
    "                           log_model=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/girish/miniconda3/envs/minerva/lib/python3.9/site-packages/lightning_fabric/connector.py:554: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "\n",
    "trainer = pl.Trainer(fast_dev_run=False,            # For debugging purposes\n",
    "                    log_every_n_steps=1,           # set the logging frequency\n",
    "                    accelerator='gpu',            # Precedence: tpu > gpu >> cpu\n",
    "                    devices=[1],                       # all\n",
    "                    # strategy=\"ddp_notebook\",       # distributed data parallel\n",
    "                    max_epochs=CONFIG['epochs'],   # number of epochs\n",
    "                    precision=16,\n",
    "                    callbacks=[TQDMProgressBar(refresh_rate=25), \n",
    "                               checkpoint_callback, \n",
    "                               lr_monitor],\n",
    "                    logger=wandb_logger,           # wandb <3\n",
    "                    benchmark=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/girish/miniconda3/envs/minerva/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:196: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name  | Type               | Params\n",
      "---------------------------------------------\n",
      "0 | model | FullyConnected     | 154 M \n",
      "1 | roc   | MulticlassROC      | 0     \n",
      "2 | acc   | MulticlassAccuracy | 0     \n",
      "3 | auroc | MulticlassAUROC    | 0     \n",
      "4 | f1    | MulticlassF1Score  | 0     \n",
      "---------------------------------------------\n",
      "154 M     Trainable params\n",
      "0         Non-trainable params\n",
      "154 M     Total params\n",
      "616.595   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5125caa8a4d48158dbc4b8709e160de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/girish/miniconda3/envs/minerva/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Train model:\n",
    "model = LIT_Scratch(fcn)\n",
    "trainer.fit(model, trainLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"
     ]
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minerva",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
