{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚡ Lightning + WandB - Custom Scratch Models (CNN & Fully Connected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "\n",
    "import wandb\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities.model_summary import ModelSummary\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import torchmetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maryangarg019\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder Utilities ----------------------------\n",
    "\n",
    "## Create dir if it doesn't exist\n",
    "def create_dir(dir_name):\n",
    "  if not os.path.exists(f'{dir_name}'):\n",
    "    os.mkdir(f'{dir_name}')\n",
    "\n",
    "## Delete dir: checkpoints\n",
    "def delete_dir(dir_name):\n",
    "  if os.path.isdir(f'{dir_name}'):\n",
    "    shutil.rmtree(f'{dir_name}')\n",
    "\n",
    "# ---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: For experiments later on!\n",
    "# train_transform = A.Compose(\n",
    "#     [\n",
    "#         A.SmallestMaxSize(max_size=160),\n",
    "#         A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "#         A.RandomCrop(height=128, width=128),\n",
    "#         A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
    "#         A.RandomBrightnessContrast(p=0.5),\n",
    "#         A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "#         ToTensorV2(),\n",
    "#     ]\n",
    "# )\n",
    "train_transform = T.Compose([T.ToTensor(), T.Resize((224, 224))])\n",
    "test_transform = T.Compose([T.ToTensor(), T.Resize((224, 224))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'mode': 'train',\n",
    "    'train_path': 'dataset_FER/train/',\n",
    "    'test_path': 'dataset_FER/test/',\n",
    "    'epochs': 10,\n",
    "    'batch_size': 16,\n",
    "    'lr': 0.001,\n",
    "    'num_workers': 4,\n",
    "    'device': 'cuda',\n",
    "    'device_ids': [0,1],\n",
    "    'load_model': False,\n",
    "    'checkpoint_path': 'ckpts/scratch_CNN/',\n",
    "    'save_every': 10,\n",
    "    'device': 'cuda:1' if torch.cuda.is_available() else 'cpu',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.ImageFolder(root='dataset_FER/train/', transform=train_transform)\n",
    "test_data = os.listdir('dataset_FER/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoader = DataLoader(train_data, batch_size=16, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 224, 224]),\n",
       " tensor([4, 5, 0, 4, 2, 2, 4, 3, 2, 5, 3, 3, 2, 3, 6, 3]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample, y = next(iter(trainLoader))\n",
    "sample.shape, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.scratch_cnn import ScratchCNN\n",
    "from models.scratch_fully_connected import FullyConnected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn = ScratchCNN()\n",
    "# cnn.to('cuda')\n",
    "\n",
    "# from torchsummary import summary\n",
    "# summary(cnn, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = ScratchCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIT Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIT_Scratch(pl.LightningModule):\n",
    "  \n",
    "  def __init__(self, model):\n",
    "    super().__init__()\n",
    "    self.model = model\n",
    "    self.save_hyperparameters()\n",
    "\n",
    "    self.roc = torchmetrics.ROC(task='multiclass', num_classes=7)\n",
    "    self.acc = torchmetrics.Accuracy(task='multiclass', num_classes=7)\n",
    "    self.auroc = torchmetrics.classification.MulticlassAUROC(num_classes=7)\n",
    "    self.f1 = torchmetrics.F1Score(task='multiclass', num_classes=7)\n",
    "\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    optimizer = torch.optim.AdamW(self.parameters(), lr=3e-4, weight_decay=1e-3, betas=(0.9, 0.999), eps=1e-8)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "  def forward(self, z):\n",
    "    return self.model(z)\n",
    "\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    imgs, y = batch\n",
    "    y_hat = self(imgs)\n",
    "    loss = F.cross_entropy(y_hat, y)\n",
    "\n",
    "    self.log('train_loss_CE', loss, prog_bar=True)\n",
    "    \n",
    "    # np_y = y.clone().detach().view(-1).cpu().numpy()\n",
    "    # np_yhat = y_hat.clone().detach().view(-1).cpu().numpy()\n",
    "\n",
    "    # metrics\n",
    "    self.log('train_acc', self.acc(y_hat, y), prog_bar=True)\n",
    "    self.log('train_F1', self.f1(y_hat, y), prog_bar=False)\n",
    "\n",
    "    self.roc.update(y_hat, y)\n",
    "    # print(f\"{batch_idx+1} ROC --> FPR: {fpr} | TPR: {tpr}\")\n",
    "\n",
    "    # self.log({\"ROC\" : wandb.plot.roc_curve(np_y, np_yhat, \\\n",
    "    #   labels=[\"Angry\",\"Disgust\",\"Fear\",\"Happy\",\"Neutral\",\"Sad\",\"Surprise\"], classes_to_plot=None)}\n",
    "    #   , prog_bar=False)\n",
    "    \n",
    "    # wandb.sklearn.plot_confusion_matrix(np_y, np_yhat, [\"Angry\",\"Disgust\",\"Fear\",\"Happy\",\"Neutral\",\"Sad\",\"Surprise\"]) \n",
    "    \n",
    "    # self.log({\"Precision-Recall\": wandb.plot.pr_curve(np_y, np_yhat, \\\n",
    "    #   labels=[\"Angry\",\"Disgust\",\"Fear\",\"Happy\",\"Neutral\",\"Sad\",\"Surprise\"], classes_to_plot=None)})\n",
    "\n",
    "    self.log('train_AUROC', self.auroc(y_hat, y), prog_bar=False)\n",
    "\n",
    "    return loss\n",
    "    \n",
    "\n",
    "  def on_train_epoch_end(self):\n",
    "    roc = self.roc.compute()\n",
    "    print( f\"len(roc): {len(roc)}\")\n",
    "    self.roc.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Callback\n",
    "from pytorch_lightning.callbacks import DeviceStatsMonitor, TQDMProgressBar, ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=CONFIG['checkpoint_path'],\n",
    "                                      filename='{epoch}-{train_loss_CE:.2f}',\n",
    "                                      monitor='train_loss_CE',\n",
    "                                      save_top_k=2,\n",
    "                                      save_last=True,\n",
    "                                      save_weights_only=True,\n",
    "                                      verbose=True,\n",
    "                                      mode='min')\n",
    "\n",
    "# Exp2: Learning Rate Monitor\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step', log_momentum=True)\n",
    "\n",
    "# Earlystopping\n",
    "# earlystopping = EarlyStopping(monitor='train_loss_CE', patience=3, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230801_130543-9hk5ctdc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aryangarg019/MMU-FER/runs/9hk5ctdc' target=\"_blank\">CNN_10epochs</a></strong> to <a href='https://wandb.ai/aryangarg019/MMU-FER' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aryangarg019/MMU-FER' target=\"_blank\">https://wandb.ai/aryangarg019/MMU-FER</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aryangarg019/MMU-FER/runs/9hk5ctdc' target=\"_blank\">https://wandb.ai/aryangarg019/MMU-FER/runs/9hk5ctdc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(project='MMU-FER', \n",
    "                           name='CNN_10epochs',\n",
    "                           config=CONFIG,\n",
    "                           job_type='train',\n",
    "                           log_model=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/girish/miniconda3/envs/minerva/lib/python3.9/site-packages/lightning_fabric/connector.py:554: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "\n",
    "trainer = pl.Trainer(fast_dev_run=False,            # For debugging purposes\n",
    "                    log_every_n_steps=1,           # set the logging frequency\n",
    "                    accelerator='gpu',            # Precedence: tpu > gpu >> cpu\n",
    "                    devices=[1],                       # all\n",
    "                    # strategy=\"ddp_notebook\",       # distributed data parallel\n",
    "                    max_epochs=CONFIG['epochs'],   # number of epochs\n",
    "                    precision=16,\n",
    "                    callbacks=[TQDMProgressBar(refresh_rate=25), \n",
    "                               checkpoint_callback, \n",
    "                               lr_monitor],\n",
    "                    logger=wandb_logger,           # wandb <3\n",
    "                    benchmark=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/girish/miniconda3/envs/minerva/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:196: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "  rank_zero_warn(\n",
      "/home/girish/miniconda3/envs/minerva/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /data2/aryan/MMU/ckpts/scratch_CNN exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name  | Type               | Params\n",
      "---------------------------------------------\n",
      "0 | model | ScratchCNN         | 51.9 M\n",
      "1 | roc   | MulticlassROC      | 0     \n",
      "2 | acc   | MulticlassAccuracy | 0     \n",
      "3 | auroc | MulticlassAUROC    | 0     \n",
      "4 | f1    | MulticlassF1Score  | 0     \n",
      "---------------------------------------------\n",
      "51.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "51.9 M    Total params\n",
      "207.733   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598db05e48084c28a83dc0dd2712ee35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/girish/miniconda3/envs/minerva/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Train model:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m LIT_Scratch(cnn)\n\u001b[0;32m----> 3\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, trainLoader)\n",
      "File \u001b[0;32m~/miniconda3/envs/minerva/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:529\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    527\u001b[0m model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    528\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 529\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    530\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    531\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/minerva/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:42\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     44\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     45\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/miniconda3/envs/minerva/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:568\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[1;32m    559\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[1;32m    560\u001b[0m )\n\u001b[1;32m    562\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    563\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    564\u001b[0m     ckpt_path,\n\u001b[1;32m    565\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    566\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    567\u001b[0m )\n\u001b[0;32m--> 568\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    570\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    571\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/minerva/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:973\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    970\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 973\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    975\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    977\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    978\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/minerva/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1016\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1015\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1016\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1017\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected state \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/minerva/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:202\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance()\n\u001b[0;32m--> 202\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_advance_end()\n\u001b[1;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/minerva/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:368\u001b[0m, in \u001b[0;36m_FitLoop.on_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[39m# call train epoch end hooks\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[39m# we always call callback hooks first, but here we need to make an exception for the callbacks that\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[39m# monitor a metric, otherwise they wouldn't be able to monitor a key logged in\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39m# `LightningModule.on_train_epoch_end`\u001b[39;00m\n\u001b[1;32m    367\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(trainer, \u001b[39m\"\u001b[39m\u001b[39mon_train_epoch_end\u001b[39m\u001b[39m\"\u001b[39m, monitoring_callbacks\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 368\u001b[0m call\u001b[39m.\u001b[39;49m_call_lightning_module_hook(trainer, \u001b[39m\"\u001b[39;49m\u001b[39mon_train_epoch_end\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    369\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(trainer, \u001b[39m\"\u001b[39m\u001b[39mon_train_epoch_end\u001b[39m\u001b[39m\"\u001b[39m, monitoring_callbacks\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    371\u001b[0m trainer\u001b[39m.\u001b[39m_logger_connector\u001b[39m.\u001b[39mon_epoch_end()\n",
      "File \u001b[0;32m~/miniconda3/envs/minerva/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:144\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[1;32m    143\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 144\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    146\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    147\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "Cell \u001b[0;32mIn[13], line 56\u001b[0m, in \u001b[0;36mLIT_Scratch.on_train_epoch_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_epoch_end\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     55\u001b[0m   roc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroc\u001b[39m.\u001b[39mcompute()\n\u001b[0;32m---> 56\u001b[0m   \u001b[39mprint\u001b[39m( \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mROC.shape: \u001b[39m\u001b[39m{\u001b[39;00mroc\u001b[39m.\u001b[39;49mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroc\u001b[39m.\u001b[39mreset()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Train model:\n",
    "model = LIT_Scratch(cnn)\n",
    "trainer.fit(model, trainLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr-AdamW</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr-AdamW-momentum</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_AUROC</td><td>▅▄▁▆▁█▁▅▅▁▅▁▅██▅▁▁▅▅█▅▅▅▁▁▁▁▁▅▅██▅▁▅▅▅▁▁</td></tr><tr><td>train_F1</td><td>▃▄▅▂▁▆█▂▆▅▅▂▄▃▃▂▄▄▅▄▄▄▅▄▇▂█▅▅▂▃▄▂▅▄▃▃▄▅▄</td></tr><tr><td>train_acc</td><td>▃▄▅▂▁▆█▂▆▅▅▂▄▃▃▂▄▄▅▄▄▄▅▄▇▂█▅▅▂▃▄▂▅▄▃▃▄▅▄</td></tr><tr><td>train_loss_CE</td><td>▆▅▅▇█▃▁▇▃▅▅▇▅▆▆▇▅▅▅▅▅▅▅▅▂▇▁▅▅▇▆▅▇▄▅▆▆▅▅▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>lr-AdamW</td><td>0.001</td></tr><tr><td>lr-AdamW-momentum</td><td>0.9</td></tr><tr><td>train_AUROC</td><td>0.42857</td></tr><tr><td>train_F1</td><td>0.3</td></tr><tr><td>train_acc</td><td>0.3</td></tr><tr><td>train_loss_CE</td><td>1.86542</td></tr><tr><td>trainer/global_step</td><td>2227</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">CNN</strong> at: <a href='https://wandb.ai/aryangarg019/MMU-FER/runs/i5b4celf' target=\"_blank\">https://wandb.ai/aryangarg019/MMU-FER/runs/i5b4celf</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230801_130116-i5b4celf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minerva",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
